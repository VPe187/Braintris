# NeuralNetwork osztály konstansai
FILE_NAME = brain.dat
FEED_DATA_SIZE = 4
OUTPUT_NODES = 1
NORMALIZE_FEED_DATA = false

# MINMAX / ZSCORE
FEED_DATA_NORMALIZER = MINMAX
LAYER_NAMES = INP,H1,H2,H3, OUT
LAYER_SIZES = ${FEED_DATA_SIZE},64,64,${OUTPUT_NODES}

# SIGMOID, TANH, RELU, LEAKY_RELU, ELU, GELU, LINEAR, SWISH, MISH, SOFTMAX, SOFTMAX_SPLIT
LAYER_ACTIVATIONS = RELU,RELU,LINEAR

# RANDOM, XAVIER, HE, UNIFORM, ZERO
WEIGHT_INIT_STRATEGIES = XAVIER,XAVIER,XAVIER

# INP-H1,H1-H2,H2-H3,H3-OUT
MINIMUM_BATCH_SIZE = 512
BATCH_NORMS = false:1.0:0.0,false:1.0:0.0,false:1.0:0.0
BATCH_DEFAULT_EPSILON = 1e-6
BATCH_DEFAULT_MOMENTUM = 0.9

# INP, H1, H2, H3, H4, OUT
L2_REGULARIZATION = 0.0001,0.00005,0.00005,0.0

# Adam
BETA1_MOMENTUM = 0.9
BETA2_RMSPROP = 0.99
ADAM_EPSILON = 1e-6

# Clipping
GRADIENT_SCALE = 1.085
CLIP_MIN = -1.0
CLIP_MAX = 1.0
CLIP_NORM = 1.0

# Learning
INITIAL_LEARNING_RATE = 0.002
LEARNING_RATE_DECAY = 0.9999
MIN_LEARNING_RATE = 0.0001

# Discount
INITIAL_DISCOUNT_FACTOR = 0.99 
MAX_DISCOUNT_FACTOR = 0.99
DISCOUNT_FACTOR_INCREMENT = 0.0

# Epsylon Greedy
INITIAL_EPSILON = 0.0
EPSILON_DECAY = 0.999
MIN_EPSILON = 0.00001

# Q Clipping
MIN_Q = -10.0
MAX_Q = 10.0

# Experinece replay
USE_EXPERIENCE = true
EXPERIENCE_REPLAY_CAPACITY = 30000
EXPERIENCE_BATCH_SIZE = 512

# Moving average
MOVING_AVERAGE_WINDOW = 1000

# Positive reward
REWARD_FULLROW = 50
REWARD_NEARLY_FULLROW = 30
REWARD_PLACE_WITHOUT_HOLE = 40
REWARD_DROP_LOWER = 30

# Negative reward
REWARD_DROPPED_ELEMENTS = 0.01
REWARD_AVG_DENSITY = 10
REWARD_DROP_HIGHER = 0.5
REWARD_NUMBER_OF_HOLES = 0.5
REWARD_SURROUNDED_HOLES = 0.5
REWARD_BLOCKED_ROW = 0.2
REWARD_BUMPINESS = 0.2
REWARD_AVG_COLUMN_HEIGHT = 5
REWARD_MAXIMUM_HEIGHT = 0.2