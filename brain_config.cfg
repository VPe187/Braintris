# Futási mód (HUMAN, TRAIN_AI, PLAY_AI)
RUNMODE = TRAIN_AI

# Hálózat:
FILE_NAME = brain.dat
FEED_DATA_SIZE = 18
OUTPUT_NODES = 1
LAYER_NAMES = INP,H1,H2,H3,OUT
LAYER_SIZES = ${FEED_DATA_SIZE},64,64,64,${OUTPUT_NODES}

#Aktiváció (SIGMOID, TANH, RELU, LEAKY_RELU, ELU, GELU, LINEAR, SWISH, MISH, SOFTMAX, SOFTMAX_SPLIT):
LAYER_ACTIVATIONS = ELU,ELU,ELU,LINEAR

#Súly init (RANDOM, XAVIER, HE, UNIFORM, ZERO):
WEIGHT_INIT_STRATEGIES = HE,HE,HE,HE

# Input normalizálás (MINMAX / ZSCORE):
NORMALIZE_FEED_DATA = false
FEED_DATA_NORMALIZER = MINMAX

# Batch normalizáció (INP-H1, H1-H2, H2-H3, H3-OUT):
BATCH_NORMS = false:1.0:0.0,false:1.0:0.0,false:1.0:0.0,false:1.0:0.0
MINIMUM_BATCH_SIZE = 128
BATCH_DEFAULT_EPSILON = 1e-6
BATCH_DEFAULT_MOMENTUM = 0.9

# Lambda 2 regularizáció INP, H1, H2, H3, H4, H5, OUT:
L2_REGULARIZATION = 0.0,0.00001,0.00001,0.00001,0.0
BIAS_L2_LAMBDA = 0.00001

# Adam optimizer:
BETA1_MOMENTUM = 0.9
BETA2_RMSPROP = 0.999
ADAM_EPSILON = 1e-6

# Clipping:
GRADIENT_SCALE = 1.0
CLIP_MIN = -0.5
CLIP_MAX = 0.5
CLIP_NORM = 1.0

# Learning:
INITIAL_LEARNING_RATE = 0.001
LEARNING_RATE_DECAY = 0.99995
MIN_LEARNING_RATE = 0.0005

# Q Learning
INITIAL_Q_LEARNING_RATE = 0.01
Q_LEARNING_RATE_DECAY = 0.99995
MIN_Q_LEARNING_RATE = 0.001

# Discount:
INITIAL_DISCOUNT_FACTOR = 0.0
MAX_DISCOUNT_FACTOR = 0.5
DISCOUNT_FACTOR_INCREMENT = 0.00005

# Epsylon Greedy:
INITIAL_EPSILON = 0.6
EPSILON_DECAY = 0.995
MIN_EPSILON = 0.0001

# Q Clipping:
MIN_Q = -50.0
MAX_Q = 50.0

# Experinece replay:
USE_EXPERIENCE = true
EXPERIENCE_REPLAY_CAPACITY = 20000
EXPERIENCE_BATCH_SIZE = 128

# Moving average:
MOVING_AVERAGE_WINDOW = 1000

# Metric points:
POINT_FULLROW = 0.760666
POINT_HEIGHTS = 0.410066
POINT_HOLES = 0.35663
POINT_BUMPINESS = 0.184483

#POINT_FULLROW = 0.760666
#POINT_HEIGHTS = 0.410066
#POINT_HOLES = 0.35663
#POINT_BUMPINESS = 0.184483