# Futási mód (HUMAN, TRAIN_AI, PLAY_AI)
RUNMODE = TRAIN_AI

# Hálózat:
FILE_NAME = brain.dat
FEED_DATA_SIZE = 18
OUTPUT_NODES = 1
LAYER_NAMES = INP,H1,H2,H3,OUT
LAYER_SIZES = ${FEED_DATA_SIZE},64,64,${OUTPUT_NODES}

#Aktiváció (SIGMOID, TANH, RELU, LEAKY_RELU, ELU, GELU, LINEAR, SWISH, MISH, SOFTMAX, SOFTMAX_SPLIT):
LAYER_ACTIVATIONS = ELU,ELU,LINEAR

#Súly init (RANDOM, XAVIER, HE, UNIFORM, ZERO):
WEIGHT_INIT_STRATEGIES = HE,HE,HE

# Input normalizálás (MINMAX / ZSCORE):
NORMALIZE_FEED_DATA = false
FEED_DATA_NORMALIZER = MINMAX

# Batch normalizáció (INP-H1, H1-H2, H2-H3, H3-OUT):
BATCH_NORMS = false:1.0:0.0,false:1.0:0.0,false:1.0:0.0
MINIMUM_BATCH_SIZE = 128
BATCH_DEFAULT_EPSILON = 1e-8
BATCH_DEFAULT_MOMENTUM = 0.9

# Lambda 2 regularizáció INP, H1, H2, H3, H4, H5, OUT:
L2_REGULARIZATION = 0.00001,0.00001,0.00001,0.0
BIAS_L2_LAMBDA = 0.00001

# Adam optimizer:
BETA1_MOMENTUM = 0.9
BETA2_RMSPROP = 0.999
ADAM_EPSILON = 1e-8

# Clipping:
GRADIENT_SCALE = 0.6
CLIP_MIN = -0.6
CLIP_MAX = 0.6
CLIP_NORM = 1.0

# Dropout
DROPOUT_RATE = 0.25

# Learning:
INITIAL_LEARNING_RATE = 0.001
LEARNING_RATE_DECAY = 0.999
MIN_LEARNING_RATE = 0.00001

# Q Learning
INITIAL_Q_LEARNING_RATE = 0.01
Q_LEARNING_RATE_DECAY = 0.99995
MIN_Q_LEARNING_RATE = 0.001

# Discount:
INITIAL_DISCOUNT_FACTOR = 0.0
MAX_DISCOUNT_FACTOR = 0.99
DISCOUNT_FACTOR_INCREMENT = 0.001

# Epsylon Greedy:
INITIAL_EPSILON = 1
EPSILON_DECAY = 0.992
MIN_EPSILON = 0.0

# Q Clipping:
MIN_Q = -50.0
MAX_Q = 50.0

# Experinece replay:
USE_EXPERIENCE = true
EXPERIENCE_REPLAY_CAPACITY = 20000
EXPERIENCE_BATCH_SIZE = 128

# Moving average:
MOVING_AVERAGE_WINDOW = 1000

# Metric points:
POINT_FULLROW = 0.66
POINT_HEIGHTS = 0.51
POINT_HOLES = 0.39
POINT_BUMPINESS = 0.28

#POINT_FULLROW = 0.76
#POINT_HEIGHTS = 0.51
#POINT_HOLES = 0.36
#POINT_BUMPINESS = 0.18

