# Futási mód (HUMAN, TRAIN_AI, PLAY_AI)
RUNMODE = TRAIN_AI

# Hálózat:
FILE_NAME = brain.dat
FEED_DATA_SIZE = 4
OUTPUT_NODES = 1
NORMALIZE_FEED_DATA = false

# Input normalizálás (MINMAX / ZSCORE):
FEED_DATA_NORMALIZER = ZSCORE
LAYER_NAMES = INP,H1,H2,OUT
LAYER_SIZES = ${FEED_DATA_SIZE},64,64,${OUTPUT_NODES}

#Aktiváció (SIGMOID, TANH, RELU, LEAKY_RELU, ELU, GELU, LINEAR, SWISH, MISH, SOFTMAX, SOFTMAX_SPLIT):
LAYER_ACTIVATIONS = RELU,RELU,LINEAR

#Súly init (RANDOM, XAVIER, HE, UNIFORM, ZERO):
WEIGHT_INIT_STRATEGIES = HE,HE,XAVIER

# Batch normalizáció (INP-H1,H1-H2,H2-H3,H3-OUT):
BATCH_NORMS = false:1.0:0.0,false:1.0:0.0,false:1.0:0.0
MINIMUM_BATCH_SIZE = 64
BATCH_DEFAULT_EPSILON = 1e-6
BATCH_DEFAULT_MOMENTUM = 0.9

# Lambda 2 regularizáció INP, H1, H2, H3, H4, OUT:
L2_REGULARIZATION = 0.0001,0.0001,0.0001,0.0001
BIAS_L2_LAMBDA = 0.00001

# Adam optimizer:
BETA1_MOMENTUM = 0.9
BETA2_RMSPROP = 0.999
ADAM_EPSILON = 1e-6

# Clipping:
GRADIENT_SCALE = 2.0
CLIP_MIN = -1.0
CLIP_MAX = 1.0
CLIP_NORM = 1.0

# Learning:
INITIAL_LEARNING_RATE = 0.001
LEARNING_RATE_DECAY = 0.9995
MIN_LEARNING_RATE = 0.0001

# Q Learning
INITIAL_Q_LEARNING_RATE = 0.1
Q_LEARNING_RATE_DECAY = 0.9995
MIN_Q_LEARNING_RATE = 0.001

# Discount:
INITIAL_DISCOUNT_FACTOR = 0.1
MAX_DISCOUNT_FACTOR = 0.9
DISCOUNT_FACTOR_INCREMENT = 0.0005

# Epsylon Greedy:
INITIAL_EPSILON = 0.9
EPSILON_DECAY = 0.995
MIN_EPSILON = 0.01

# Q Clipping:
MIN_Q = -5000.0
MAX_Q = 5000.0

# Experinece replay:
USE_EXPERIENCE = true
EXPERIENCE_REPLAY_CAPACITY = 20000
EXPERIENCE_BATCH_SIZE = 64

# Moving average:
MOVING_AVERAGE_WINDOW = 1000

# Metric points:
POINT_FULLROW = 0.760666
POINT_HEIGHTS = 0.410066
POINT_HOLES = 0.35663
POINT_BUMPINESS = 0.184483